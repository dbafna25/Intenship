{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0853c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aaa\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aaa\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26f883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Headers\n",
       "0           Welcome to Wikipedia\n",
       "1  From today's featured article\n",
       "2               Did you know ...\n",
       "3                    In the news\n",
       "4                    On this day\n",
       "5       Today's featured picture\n",
       "6       Other areas of Wikipedia\n",
       "7    Wikipedia's sister projects\n",
       "8            Wikipedia languages"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page= requests.get('https://en.wikipedia.org/wiki/Main_Page') \n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "header= [] \n",
    "for i in soup.find_all('span',class_=\"mw-headline\"):\n",
    "    header.append(i.text)\n",
    "\n",
    "df=pd.DataFrame({'Headers':header})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9e5a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Closing Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January 1950</td>\n",
       "      <td>13 May 1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May 1962</td>\n",
       "      <td>13 May 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Zakir Hussain</td>\n",
       "      <td>13 May 1967</td>\n",
       "      <td>3 May 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Varahagiri Venkata Giri</td>\n",
       "      <td>3 May 1969</td>\n",
       "      <td>20 July 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Varahagiri Venkata Giri</td>\n",
       "      <td>24 August 1969</td>\n",
       "      <td>24 August 1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August 1974</td>\n",
       "      <td>11 February 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July 1977</td>\n",
       "      <td>25 July 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Giani Zali Singh</td>\n",
       "      <td>25 July 1982</td>\n",
       "      <td>25 July 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ramaswamy Venkataraman</td>\n",
       "      <td>25 July 1987</td>\n",
       "      <td>25 July 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shankar Dayal Sharma</td>\n",
       "      <td>25 July 1992</td>\n",
       "      <td>25 July 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kocheril Raman Narayanan</td>\n",
       "      <td>25 July 1997</td>\n",
       "      <td>25 July 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July 2002</td>\n",
       "      <td>25 July 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pratibha Patil</td>\n",
       "      <td>25 July 2007</td>\n",
       "      <td>25 July 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pranab Mukherjee</td>\n",
       "      <td>25 July 2012</td>\n",
       "      <td>25 July 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July 2017</td>\n",
       "      <td>21 July 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Draupadi Murmu</td>\n",
       "      <td>21 July 2022</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Names       Start Date      Closing Date\n",
       "0                                                                  \n",
       "1            Dr. Rajendra Prasad  26 January 1950       13 May 1962\n",
       "2   Dr. Sarvepalli Radhakrishnan      13 May 1962       13 May 1967\n",
       "3              Dr. Zakir Hussain      13 May 1967        3 May 1969\n",
       "4        Varahagiri Venkata Giri       3 May 1969      20 July 1969\n",
       "5        Varahagiri Venkata Giri   24 August 1969    24 August 1974\n",
       "6           Fakhruddin Ali Ahmed   24 August 1974  11 February 1977\n",
       "7           Neelam Sanjiva Reddy     25 July 1977      25 July 1982\n",
       "8               Giani Zali Singh     25 July 1982      25 July 1987\n",
       "9         Ramaswamy Venkataraman     25 July 1987      25 July 1992\n",
       "10          Shankar Dayal Sharma     25 July 1992      25 July 1997\n",
       "11      Kocheril Raman Narayanan     25 July 1997      25 July 2002\n",
       "12        Dr. A.P.J. Abdul Kalam     25 July 2002      25 July 2007\n",
       "13                Pratibha Patil     25 July 2007      25 July 2012\n",
       "14              Pranab Mukherjee     25 July 2012      25 July 2017\n",
       "15          Shri Ram Nath Kovind     25 July 2017      21 July 2022\n",
       "16                Draupadi Murmu     21 July 2022           Working"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://currentaffairs.adda247.com/list-of-presidents-of-india/ and make data frame.\n",
    "\n",
    "# NOTE:the given link in Q is showing 'page not found'. So for name & image.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#for Name\n",
    "page= requests.get('https://currentaffairs.adda247.com/list-of-presidents-of-india/')\n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "name=[]\n",
    "for i in soup.find_all ('td',width=\"220\"):\n",
    "    name.append(i.text.replace('\\nName\\n',''))\n",
    "\n",
    "    \n",
    "#for Term of office    \n",
    "page2= requests.get('https://currentaffairs.adda247.com/list-of-presidents-of-india/') #if both the datas('name' and 'detaisl') are taken from the same link then no need to mention it twice\n",
    "\n",
    "soup2= BeautifulSoup(page2.content) #if both the datas('name' and 'detaisl') are taken from the same link then no need to mention it twice\n",
    "   \n",
    "detail1=[]\n",
    "for i in soup2.find_all ('td',width=\"180\"):\n",
    "    detail1.append(i.text.replace('\\nStart Date\\n',''))    \n",
    "\n",
    "    \n",
    "detail2=[]\n",
    "for i in soup2.find_all ('td',width=\"201\"):\n",
    "    detail2.append(i.text.replace('Closing date','')) \n",
    "    \n",
    "\n",
    "#printing length\n",
    "print(len(name),len(detail1),len(detail2)) #both columns should have same number of datas to make a dataframe\n",
    "\n",
    "\n",
    "# Making dataframe\n",
    "df=pd.DataFrame({'Names':name, 'Start Date':detail1, 'Closing Date':detail2})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb19802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 99 99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shubman Gill</td>\n",
       "      <td>IND</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Tector</td>\n",
       "      <td>IRE</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fakhar Zaman</td>\n",
       "      <td>PAK</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temba Bavuma</td>\n",
       "      <td>SA</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Batsmen player Name Team Rating\n",
       "0           Shubman Gill  IND    759\n",
       "1  Rassie van der Dussen   SA    745\n",
       "2           David Warner  AUS    739\n",
       "3            Imam-ul-Haq  PAK    735\n",
       "4           Harry Tector  IRE    726\n",
       "5        Quinton de Kock   SA    721\n",
       "6            Virat Kohli  IND    715\n",
       "7           Rohit Sharma  IND    707\n",
       "8           Fakhar Zaman  PAK    705\n",
       "9           Temba Bavuma   SA    691"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3) b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "\n",
    "\n",
    "\n",
    "page= requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting') \n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "player= [] \n",
    "team=[]\n",
    "rating= [] \n",
    "\n",
    "\n",
    "#player\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player.append(i.text.replace('\\n',''))\n",
    "\n",
    "\n",
    "#team names\n",
    "team_name= [] \n",
    "for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    team.append(i.text)\n",
    "\n",
    "\n",
    "# rating\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "print(len(player), len(team),len(rating)) # checking\n",
    "\n",
    " \n",
    "    \n",
    "#Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'Batsmen player Name':player[0:10], 'Team':team[:10], 'Rating':rating[:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99db18cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 99 99\n",
      "99 99 99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batting player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batting player Name Team Rating\n",
       "0          Beth Mooney  AUS    751\n",
       "1  Chamari Athapaththu   SL    743\n",
       "2      Laura Wolvaardt   SA    708\n",
       "3      Smriti Mandhana  IND    708\n",
       "4         Alyssa Healy  AUS    702\n",
       "5     Harmanpreet Kaur  IND    694\n",
       "6         Ellyse Perry  AUS    686\n",
       "7          Meg Lanning  AUS    682\n",
       "8      Stafanie Taylor   WI    618\n",
       "9       Marizanne Kapp   SA    617"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3) c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "\n",
    "page= requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling') \n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "player= [] \n",
    "team=[]\n",
    "rating= [] \n",
    "\n",
    "\n",
    "#player\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player.append(i.text.replace('\\n',''))\n",
    "\n",
    "\n",
    "#team names\n",
    "team_name= [] \n",
    "for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    team.append(i.text)\n",
    "\n",
    "\n",
    "# rating\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "print(len(player), len(team),len(rating)) # checking\n",
    "\n",
    " \n",
    "    \n",
    "#Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'Bowlers player Name':player[0:10], 'Team':team[:10], 'Rating':rating[:10]})\n",
    "df#4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "\n",
    "\n",
    "\n",
    "page= requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting') \n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "player= [] \n",
    "team=[]\n",
    "rating= [] \n",
    "\n",
    "\n",
    "#player\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player.append(i.text.replace('\\n',''))\n",
    "\n",
    "\n",
    "#team names\n",
    "team_name= [] \n",
    "for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    team.append(i.text)\n",
    "\n",
    "\n",
    "# rating\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "print(len(player), len(team),len(rating)) # checking\n",
    "\n",
    " \n",
    "    \n",
    "#Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'Batting player Name':player[0:10], 'Team':team[:10], 'Rating':rating[:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dda20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 19 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all rounder player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sophie Devine</td>\n",
       "      <td>NZ</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nida Dar</td>\n",
       "      <td>PAK</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all rounder player Name Team Rating\n",
       "0        Ashleigh Gardner  AUS    389\n",
       "1         Hayley Matthews   WI    382\n",
       "2          Marizanne Kapp   SA    362\n",
       "3            Ellyse Perry  AUS    329\n",
       "4             Amelia Kerr   NZ    328\n",
       "5           Deepti Sharma  IND    312\n",
       "6           Jess Jonassen  AUS    241\n",
       "7           Sophie Devine   NZ    233\n",
       "8                Nida Dar  PAK    217\n",
       "9       Sophie Ecclestone  ENG    196"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4) c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "page= requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder') \n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "player= [] \n",
    "team=[]\n",
    "rating= [] \n",
    "\n",
    "\n",
    "#player\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "    player.append(i.text.replace('\\n',''))\n",
    "\n",
    "\n",
    "#team names\n",
    "team_name= [] \n",
    "for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    team.append(i.text)\n",
    "\n",
    "\n",
    "# rating\n",
    "for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "print(len(player), len(team),len(rating)) # checking\n",
    "\n",
    " \n",
    "    \n",
    "#Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'all rounder player Name':player[0:10], 'Team':team[:10], 'Rating':rating[:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41f07fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53 Min AgoThe No. 1 personality trait linked t...</td>\n",
       "      <td>53 Min Ago</td>\n",
       "      <td>[The No. 1 personality trait linked to a long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Hours AgoThese are the 11 sectors of the S&amp;P...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>[These are the 11 sectors of the S&amp;P 500 — and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Hours AgoNo. 1 tip for starting a side hustl...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>[No. 1 tip for starting a side hustle, from 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 Hours AgoThese are the top 10 states for you...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>[These are the top 10 states for young workers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 Hours AgoWhy health insurance is poised to m...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>[Why health insurance is poised to make inflat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5 Hours AgoHollywood pays a steep price for ne...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>[Hollywood pays a steep price for never really...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 Hours AgoDisney asset sales won't break the ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>[Disney asset sales won't break the bank, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5 Hours AgoIt’s been one year since the Ethere...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>[It’s been one year since the Ethereum merge. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5 Hours AgoBehind Warren Buffett's $100 billio...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>[Behind Warren Buffett's $100 billion-plus bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5 Hours AgoOne individual investor favorite ha...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>[One individual investor favorite has endured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23 Hours AgoStellantis offers raises, inflatio...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>[Stellantis offers raises, inflation protectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23 Hours AgoStrong retail sales despite stubbo...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>[Strong retail sales despite stubborn inflatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>September 16, 20234 things the world's longest...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[4 things the world's longest-living people do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>September 16, 2023How Olipop's founders starte...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[How Olipop's founders started a soda brand br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>September 16, 2023Here are three things we're ...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[Here are three things we're watching in the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>September 16, 2023U.S. states where property t...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[U.S. states where property taxes are highest—...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>September 16, 2023The No. 1 thing successful p...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[The No. 1 thing successful parents who raise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>September 16, 2023Mark Cuban: If someone says ...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[Mark Cuban: If someone says they can make you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>September 16, 2023Top investor Jenny Harringto...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[Top investor Jenny Harrington breaks down her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>September 16, 20233 financial tips for couples...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[3 financial tips for couples moving in togeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>September 16, 2023Analysts said obesity drugs ...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[Analysts said obesity drugs would be blockbus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>September 16, 2023Analysts see stocks like Mic...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[Analysts see stocks like Microsoft offering '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>September 16, 2023Stocks churn with the S&amp;P 50...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[Stocks churn with the S&amp;P 500 sitting at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>September 16, 2023We gave the $2.1 million Rim...</td>\n",
       "      <td>September 16, 2023</td>\n",
       "      <td>[We gave the $2.1 million Rimac Nevera electri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>September 15, 2023Trump lashes out at Jack Smi...</td>\n",
       "      <td>September 15, 2023</td>\n",
       "      <td>[Trump lashes out at Jack Smith's bid for part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>September 15, 2023Cramer's Lightning Round: Se...</td>\n",
       "      <td>September 15, 2023</td>\n",
       "      <td>[Cramer's Lightning Round: Sell Joby Aviation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>September 15, 2023Cramer's week ahead: Pay att...</td>\n",
       "      <td>September 15, 2023</td>\n",
       "      <td>[Cramer's week ahead: Pay attention to the Fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>September 15, 2023Okta CEO on MGM breach: Comp...</td>\n",
       "      <td>September 15, 2023</td>\n",
       "      <td>[Okta CEO on MGM breach: Companies are under '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>September 15, 2023Jack Smith wants Trump barre...</td>\n",
       "      <td>September 15, 2023</td>\n",
       "      <td>[Jack Smith wants Trump barred from posting on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>September 15, 2023Adobe falls after posting qu...</td>\n",
       "      <td>September 15, 2023</td>\n",
       "      <td>[Adobe falls after posting quarterly results. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        News Headline                Time  \\\n",
       "0   53 Min AgoThe No. 1 personality trait linked t...          53 Min Ago   \n",
       "1   3 Hours AgoThese are the 11 sectors of the S&P...         3 Hours Ago   \n",
       "2   3 Hours AgoNo. 1 tip for starting a side hustl...         3 Hours Ago   \n",
       "3   4 Hours AgoThese are the top 10 states for you...         4 Hours Ago   \n",
       "4   4 Hours AgoWhy health insurance is poised to m...         4 Hours Ago   \n",
       "5   5 Hours AgoHollywood pays a steep price for ne...         5 Hours Ago   \n",
       "6   5 Hours AgoDisney asset sales won't break the ...         5 Hours Ago   \n",
       "7   5 Hours AgoIt’s been one year since the Ethere...         5 Hours Ago   \n",
       "8   5 Hours AgoBehind Warren Buffett's $100 billio...         5 Hours Ago   \n",
       "9   5 Hours AgoOne individual investor favorite ha...         5 Hours Ago   \n",
       "10  23 Hours AgoStellantis offers raises, inflatio...        23 Hours Ago   \n",
       "11  23 Hours AgoStrong retail sales despite stubbo...        23 Hours Ago   \n",
       "12  September 16, 20234 things the world's longest...  September 16, 2023   \n",
       "13  September 16, 2023How Olipop's founders starte...  September 16, 2023   \n",
       "14  September 16, 2023Here are three things we're ...  September 16, 2023   \n",
       "15  September 16, 2023U.S. states where property t...  September 16, 2023   \n",
       "16  September 16, 2023The No. 1 thing successful p...  September 16, 2023   \n",
       "17  September 16, 2023Mark Cuban: If someone says ...  September 16, 2023   \n",
       "18  September 16, 2023Top investor Jenny Harringto...  September 16, 2023   \n",
       "19  September 16, 20233 financial tips for couples...  September 16, 2023   \n",
       "20  September 16, 2023Analysts said obesity drugs ...  September 16, 2023   \n",
       "21  September 16, 2023Analysts see stocks like Mic...  September 16, 2023   \n",
       "22  September 16, 2023Stocks churn with the S&P 50...  September 16, 2023   \n",
       "23  September 16, 2023We gave the $2.1 million Rim...  September 16, 2023   \n",
       "24  September 15, 2023Trump lashes out at Jack Smi...  September 15, 2023   \n",
       "25  September 15, 2023Cramer's Lightning Round: Se...  September 15, 2023   \n",
       "26  September 15, 2023Cramer's week ahead: Pay att...  September 15, 2023   \n",
       "27  September 15, 2023Okta CEO on MGM breach: Comp...  September 15, 2023   \n",
       "28  September 15, 2023Jack Smith wants Trump barre...  September 15, 2023   \n",
       "29  September 15, 2023Adobe falls after posting qu...  September 15, 2023   \n",
       "\n",
       "                                            News Link  \n",
       "0   [The No. 1 personality trait linked to a long ...  \n",
       "1   [These are the 11 sectors of the S&P 500 — and...  \n",
       "2   [No. 1 tip for starting a side hustle, from 2 ...  \n",
       "3   [These are the top 10 states for young workers...  \n",
       "4   [Why health insurance is poised to make inflat...  \n",
       "5   [Hollywood pays a steep price for never really...  \n",
       "6   [Disney asset sales won't break the bank, but ...  \n",
       "7   [It’s been one year since the Ethereum merge. ...  \n",
       "8   [Behind Warren Buffett's $100 billion-plus bet...  \n",
       "9   [One individual investor favorite has endured ...  \n",
       "10  [Stellantis offers raises, inflation protectio...  \n",
       "11  [Strong retail sales despite stubborn inflatio...  \n",
       "12  [4 things the world's longest-living people do...  \n",
       "13  [How Olipop's founders started a soda brand br...  \n",
       "14  [Here are three things we're watching in the m...  \n",
       "15  [U.S. states where property taxes are highest—...  \n",
       "16  [The No. 1 thing successful parents who raise ...  \n",
       "17  [Mark Cuban: If someone says they can make you...  \n",
       "18  [Top investor Jenny Harrington breaks down her...  \n",
       "19  [3 financial tips for couples moving in togeth...  \n",
       "20  [Analysts said obesity drugs would be blockbus...  \n",
       "21  [Analysts see stocks like Microsoft offering '...  \n",
       "22  [Stocks churn with the S&P 500 sitting at the ...  \n",
       "23  [We gave the $2.1 million Rimac Nevera electri...  \n",
       "24  [Trump lashes out at Jack Smith's bid for part...  \n",
       "25     [Cramer's Lightning Round: Sell Joby Aviation]  \n",
       "26  [Cramer's week ahead: Pay attention to the Fed...  \n",
       "27  [Okta CEO on MGM breach: Companies are under '...  \n",
       "28  [Jack Smith wants Trump barred from posting on...  \n",
       "29  [Adobe falls after posting quarterly results. ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5)Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "# i) Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page= requests.get('https://www.cnbc.com/world/?region=world') \n",
    "\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "#for latest news headline\n",
    "\n",
    "news=[]\n",
    "for i in soup.find_all ('div',class_=\"LatestNews-headlineWrapper\"):\n",
    "    news.append(i.text)\n",
    "    \n",
    "\n",
    "#for time\n",
    "\n",
    "time=[]\n",
    "for i in soup.find_all ('time',class_=\"LatestNews-timestamp\"):\n",
    "    time.append(i.text)\n",
    "\n",
    "#for link\n",
    "\n",
    "link=[]\n",
    "for i in soup.find_all ('a',class_=\"LatestNews-headline\"):\n",
    "    link.append(i)\n",
    "\n",
    "#printing length to check if all the list have same lenth to proceed for data frame  \n",
    "print(len(time),len(news),len(link))\n",
    "      \n",
    "\n",
    "    \n",
    "# Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'News Headline':news, 'Time':time, 'News Link':link})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9a4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24 24 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>David Silver, Satinder Singh, Doina Precup, Ri...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>[[Reward is enough]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Tim Miller</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>[[Explanation in artificial intelligence: Insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Margaret A. Boden</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>[[Creativity and artificial intelligence]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Guni Sharon, Roni Stern, Ariel Felner, Nathan ...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>[[Conflict-based search for optimal multi-agen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knowledge graphs as tools for explainable mach...</td>\n",
       "      <td>Ilaria Tiddi, Stefan Schlobach</td>\n",
       "      <td>January 2022</td>\n",
       "      <td>[[Knowledge graphs as tools for explainable ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Henry Prakken, Giovanni Sartor</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>[[Law and logic: A review from an argumentatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Richard S. Sutton, Doina Precup, Satinder Singh</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>[[Between MDPs and semi-MDPs: A framework for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Kjersti Aas, Martin Jullum, Anders Løland</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>[[Explaining individual predictions when featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Wenhan Luo, Junliang Xing and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>[[Multiple object tracking: A literature review]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Saurabh Arora, Prashant Doshi</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>[[A survey of inverse reinforcement learning: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>Jasper van der Waa, Elisabeth Nieuwburg, Anita...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>[[Evaluating XAI: A comparison of rule-based a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Explainable AI tools for legal reasoning about...</td>\n",
       "      <td>Joe Collenette, Katie Atkinson, Trevor Bench-C...</td>\n",
       "      <td>April 2023</td>\n",
       "      <td>[[Explainable AI tools for legal reasoning abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hard choices in artificial intelligence</td>\n",
       "      <td>Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz</td>\n",
       "      <td>November 2021</td>\n",
       "      <td>[[Hard choices in artificial intelligence]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assessing the communication gap between AI mod...</td>\n",
       "      <td>Oskar Wysocki, Jessica Katharine Davies and 5 ...</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>[[Assessing the communication gap between AI m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>[[Explaining black-box classifiers using post-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Nolan Bard, Jakob N. Foerster and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>[[The Hanabi challenge: A new frontier for AI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Ron Kohavi, George H. John</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>[[Wrappers for feature subset selection]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Séverin Lemaignan, Mathieu Warnier and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>[[Artificial cognition for social human–robot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>[[A review of possible effects of cognitive bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The multifaceted impact of Ada Lovelace in the...</td>\n",
       "      <td>Luigia Carlucci Aiello</td>\n",
       "      <td>June 2016</td>\n",
       "      <td>[[The multifaceted impact of Ada Lovelace in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Robot ethics: Mapping the issues for a mechani...</td>\n",
       "      <td>Patrick Lin, Keith Abney, George Bekey</td>\n",
       "      <td>April 2011</td>\n",
       "      <td>[[Robot ethics: Mapping the issues for a mecha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Reward (Mis)design for autonomous driving</td>\n",
       "      <td>W. Bradley Knox, Alessandro Allievi and 3 more</td>\n",
       "      <td>March 2023</td>\n",
       "      <td>[[Reward (Mis)design for autonomous driving]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Planning and acting in partially observable st...</td>\n",
       "      <td>Leslie Pack Kaelbling, Michael L. Littman, Ant...</td>\n",
       "      <td>May 1998</td>\n",
       "      <td>[[Planning and acting in partially observable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What do we want from Explainable Artificial In...</td>\n",
       "      <td>Markus Langer, Daniel Oster and 6 more</td>\n",
       "      <td>July 2021</td>\n",
       "      <td>[[What do we want from Explainable Artificial ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1   Explanation in artificial intelligence: Insigh...   \n",
       "2              Creativity and artificial intelligence   \n",
       "3   Conflict-based search for optimal multi-agent ...   \n",
       "4   Knowledge graphs as tools for explainable mach...   \n",
       "5   Law and logic: A review from an argumentation ...   \n",
       "6   Between MDPs and semi-MDPs: A framework for te...   \n",
       "7   Explaining individual predictions when feature...   \n",
       "8       Multiple object tracking: A literature review   \n",
       "9   A survey of inverse reinforcement learning: Ch...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11  Explainable AI tools for legal reasoning about...   \n",
       "12            Hard choices in artificial intelligence   \n",
       "13  Assessing the communication gap between AI mod...   \n",
       "14  Explaining black-box classifiers using post-ho...   \n",
       "15  The Hanabi challenge: A new frontier for AI re...   \n",
       "16              Wrappers for feature subset selection   \n",
       "17  Artificial cognition for social human–robot in...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  The multifaceted impact of Ada Lovelace in the...   \n",
       "20  Robot ethics: Mapping the issues for a mechani...   \n",
       "21          Reward (Mis)design for autonomous driving   \n",
       "22  Planning and acting in partially observable st...   \n",
       "23  What do we want from Explainable Artificial In...   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
       "1                                         Tim Miller    February 2019   \n",
       "2                                  Margaret A. Boden      August 1998   \n",
       "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
       "4                     Ilaria Tiddi, Stefan Schlobach     January 2022   \n",
       "5                     Henry Prakken, Giovanni Sartor     October 2015   \n",
       "6    Richard S. Sutton, Doina Precup, Satinder Singh      August 1999   \n",
       "7          Kjersti Aas, Martin Jullum, Anders Løland   September 2021   \n",
       "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
       "9                      Saurabh Arora, Prashant Doshi      August 2021   \n",
       "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
       "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
       "12  Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz    November 2021   \n",
       "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
       "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
       "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
       "16                        Ron Kohavi, George H. John    December 1997   \n",
       "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
       "18   Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz        June 2021   \n",
       "19                            Luigia Carlucci Aiello        June 2016   \n",
       "20            Patrick Lin, Keith Abney, George Bekey       April 2011   \n",
       "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
       "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
       "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
       "\n",
       "                                            Paper URL  \n",
       "0                                [[Reward is enough]]  \n",
       "1   [[Explanation in artificial intelligence: Insi...  \n",
       "2          [[Creativity and artificial intelligence]]  \n",
       "3   [[Conflict-based search for optimal multi-agen...  \n",
       "4   [[Knowledge graphs as tools for explainable ma...  \n",
       "5   [[Law and logic: A review from an argumentatio...  \n",
       "6   [[Between MDPs and semi-MDPs: A framework for ...  \n",
       "7   [[Explaining individual predictions when featu...  \n",
       "8   [[Multiple object tracking: A literature review]]  \n",
       "9   [[A survey of inverse reinforcement learning: ...  \n",
       "10  [[Evaluating XAI: A comparison of rule-based a...  \n",
       "11  [[Explainable AI tools for legal reasoning abo...  \n",
       "12        [[Hard choices in artificial intelligence]]  \n",
       "13  [[Assessing the communication gap between AI m...  \n",
       "14  [[Explaining black-box classifiers using post-...  \n",
       "15  [[The Hanabi challenge: A new frontier for AI ...  \n",
       "16          [[Wrappers for feature subset selection]]  \n",
       "17  [[Artificial cognition for social human–robot ...  \n",
       "18  [[A review of possible effects of cognitive bi...  \n",
       "19  [[The multifaceted impact of Ada Lovelace in t...  \n",
       "20  [[Robot ethics: Mapping the issues for a mecha...  \n",
       "21      [[Reward (Mis)design for autonomous driving]]  \n",
       "22  [[Planning and acting in partially observable ...  \n",
       "23  [[What do we want from Explainable Artificial ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "# i) Paper Title\n",
    "# ii) Authors\n",
    "# iii) Published Date\n",
    "# iv) Paper URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page= requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "#empty list for store the scraped data \n",
    "title=[]\n",
    "author=[]\n",
    "pub_date=[]\n",
    "url=[]\n",
    "\n",
    "\n",
    "# for Paper Title\n",
    "\n",
    "for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "    title.append(i.text)\n",
    "\n",
    "#for Authors\n",
    "\n",
    "for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "    author.append(i.text)\n",
    "\n",
    "#for publish date\n",
    "\n",
    "for i in soup.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "    pub_date.append(i.text)\n",
    "\n",
    "    \n",
    "#for paper URL \n",
    "\n",
    "for i in soup.find_all('a',class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "    url.append(i)\n",
    "\n",
    "    \n",
    "    \n",
    "# Printing legth   \n",
    "print(len(title),len(author),len(pub_date), len(url))\n",
    "\n",
    "\n",
    "\n",
    "# Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'Paper Title':title, 'Authors':author, 'Published Date':pub_date, 'Paper URL':url})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11858af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 9 9 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>₹ 2,000 for 2  | Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>₹ 3,000 for 2  | Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle's Barbeque</td>\n",
       "      <td>₹ 2,000 for 2  | Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>₹ 2,400 for 2  | North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>₹ 1,700 for 2  | North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>₹ 1,800 for 2  | North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>₹ 1,900 for 2  | North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>₹ 2,200 for 2  | North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Barbeque Times</td>\n",
       "      <td>₹ 1,500 for 2  | North Indian, Continental, Ch...</td>\n",
       "      <td>M2K Corporate Park,Sector 51, Gurgaon</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant name  \\\n",
       "0                   Castle Barbeque   \n",
       "1                        Cafe Knosh   \n",
       "2                 Castle's Barbeque   \n",
       "3                       India Grill   \n",
       "4              The Barbeque Company   \n",
       "5                    Delhi Barbeque   \n",
       "6  The Monarch - Bar Be Que Village   \n",
       "7                 Indian Grill Room   \n",
       "8                The Barbeque Times   \n",
       "\n",
       "                                             Cuisine  \\\n",
       "0             ₹ 2,000 for 2  | Chinese, North Indian   \n",
       "1              ₹ 3,000 for 2  | Italian, Continental   \n",
       "2             ₹ 2,000 for 2  | Chinese, North Indian   \n",
       "3             ₹ 2,400 for 2  | North Indian, Italian   \n",
       "4             ₹ 1,700 for 2  | North Indian, Chinese   \n",
       "5                      ₹ 1,800 for 2  | North Indian   \n",
       "6                      ₹ 1,900 for 2  | North Indian   \n",
       "7             ₹ 2,200 for 2  | North Indian, Mughlai   \n",
       "8  ₹ 1,500 for 2  | North Indian, Continental, Ch...   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi       4   \n",
       "1  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "2             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "3               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
       "5     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "6  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "7   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "8              M2K Corporate Park,Sector 51, Gurgaon     4.1   \n",
       "\n",
       "                                           Image URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7) Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location\n",
    "# iv) Ratings\n",
    "# v) Image URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page= requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "#empty list for store the scraped data \n",
    "restaurant=[]\n",
    "cuisine=[]\n",
    "location=[]\n",
    "rating=[]\n",
    "image=[]\n",
    "\n",
    "# for Restaurant name\n",
    "\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    restaurant.append(i.text)\n",
    "\n",
    "\n",
    "#for Cuisine\n",
    "\n",
    "for i in soup.find_all('div',class_=\"detail-info\"):\n",
    "    cuisine.append(i.text.replace('(approx)',''))\n",
    "\n",
    "\n",
    "#for Location\n",
    "\n",
    "for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "\n",
    "\n",
    "#for Ratings \n",
    "\n",
    "for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "\n",
    "    \n",
    "#for Image URL\n",
    "\n",
    "for i in soup.find_all('img',class_=\"no-img\"):\n",
    "    image.append(i['data-src'])\n",
    "    \n",
    "    \n",
    "# Printing legth   \n",
    "print(len(restaurant),len(cuisine),len(location),len(rating),len(image))\n",
    "\n",
    "\n",
    "# Making dataframe\n",
    "\n",
    "df=pd.DataFrame({'Restaurant name':restaurant, 'Cuisine':cuisine, 'Location':location, 'Ratings':rating,'Image URL':image})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7503fe11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
